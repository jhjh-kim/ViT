{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YU3kyn-ZoCC",
        "outputId": "0aa61d5c-4c15-43d6-9d69-7cbf8fd4e736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting labml-nn\n",
            "  Downloading labml_nn-0.4.137-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting labml==0.4.168 (from labml-nn)\n",
            "  Downloading labml-0.4.168-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting labml-helpers==0.4.89 (from labml-nn)\n",
            "  Downloading labml_helpers-0.4.89-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from labml-nn) (2.4.1+cu121)\n",
            "Collecting torchtext (from labml-nn)\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from labml-nn) (0.19.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from labml-nn) (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from labml-nn) (1.26.4)\n",
            "Collecting fairscale (from labml-nn)\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitpython (from labml==0.4.168->labml-nn)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from labml==0.4.168->labml-nn) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (2024.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext->labml-nn) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext->labml-nn) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->labml-nn) (10.4.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->labml==0.4.168->labml-nn)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->labml-nn) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->labml-nn) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->labml==0.4.168->labml-nn)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading labml_nn-0.4.137-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading labml-0.4.168-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading labml_helpers-0.4.89-py3-none-any.whl (24 kB)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332106 sha256=c80e98f046f2dcd80291666384ef97739eaf7850589baf51a1b6749904811c0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: smmap, gitdb, torchtext, gitpython, fairscale, labml, labml-helpers, labml-nn\n",
            "Successfully installed fairscale-0.4.13 gitdb-4.0.11 gitpython-3.1.43 labml-0.4.168 labml-helpers-0.4.89 labml-nn-0.4.137 smmap-5.0.1 torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "%pip install labml-nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from labml_helpers.module import Module\n",
        "from labml_nn.transformers import TransformerLayer\n",
        "from labml_nn.utils import clone_module_list"
      ],
      "metadata": {
        "id": "gf5mFT3OZzhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(Module):\n",
        "    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n",
        "        super(PatchEmbeddings, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        '''x: input image tensor of shape [batch_size, channels, height, width]'''\n",
        "        x = self.conv(x) # [batch_size, d_model, patch_height, patch_width]\n",
        "        batch_size, c, p_h, p_w = x.shape\n",
        "        x = x.permute(2, 3, 0, 1)  # [patch_height, patch_width, batch_size, d_model]\n",
        "\n",
        "        # set N or seq_len as the leading dimension is a convention and computationally efficient\n",
        "        x = x.view(-1, batch_size, c) # [sequence_len(patch_num N), batch_size, d_model]\n",
        "        return x"
      ],
      "metadata": {
        "id": "4zPiA_LFaFsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnablePositionalEmbeddings(Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5_000):\n",
        "        super(LearnablePositionalEmbeddings, self).__init__()\n",
        "        # wrapping a tensor in 'nn.Parameter' makes it learnable within the Pytorch Model (by backpropagation)\n",
        "        # max_len: max_len of sequence, 1: compatible with batch processing (broadcasting)\n",
        "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        '''x: input tensor of shape [patch_num, batch_size, d_model]'''\n",
        "        pe = self.positional_encodings[:x.shape[0]]\n",
        "        return x + pe"
      ],
      "metadata": {
        "id": "0bUEbgfG77bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(Module):\n",
        "    def __init__(self, d_model: int, n_hidden: int, n_classes: int):\n",
        "        '''use [CLS] token to classify the image\n",
        "           use two linear layers and an activation function while training\n",
        "        '''\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, n_hidden)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(n_hidden, n_classes)\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        '''x: [CLS] token'''\n",
        "        x = self.activation(self.lienar1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qb-bOhjKB8fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(Module):\n",
        "    def __init__(self, transformer_layer: TransformerLayer,\n",
        "                 n_layers: int, patch_emb: PatchEmbeddings,\n",
        "                 pos_emb: LearnablePositionalEmbeddings,\n",
        "                 classification: ClassificationHead):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_emb = patch_emb\n",
        "        self.pos_emb = pos_emb\n",
        "        self.classification = classification\n",
        "        self.transformer_layers = clone_module_list(transformer_layer, n_layers)\n",
        "        # transformer_layer.size = d_model\n",
        "        self.cls_token_emb = nn.Parameter(torch.randn(1, 1, transformer_layer.size), requires_grad=True)\n",
        "        self.ln = nn.LayerNorm([transformer_layer.size])\n",
        "\n",
        "    def forward(self, x:torch.Tensor):\n",
        "        '''x: the input image tensor shape of [batch_size, channels, height, width]'''\n",
        "        x = self.patch_emb(x) # [N, batch_size, d_model]\n",
        "        # expand: creates a view(shallow copy) of singleton dimensions of a tensor\n",
        "        # -1: keep the dimension, you can set a specific dimension for a singleton-dimension of a tensor\n",
        "        # the expanded dimensions point to the same memory location\n",
        "        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)\n",
        "        x = torch.cat([cls_token_emb, x], dim=0)\n",
        "        x = self.pos_emb(x)\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x=x, mask=None)\n",
        "\n",
        "        x = x[0] # get the [CLS] token\n",
        "        x = self.ln(x)\n",
        "        x = self.classification(x) # get logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "7EMd7wX7Csvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}